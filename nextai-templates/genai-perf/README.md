<h1>GenAI-Perf</h1>

**Performance benchmarking for generative AI models.**

GenAI-Perf is a command line tool for measuring the throughput and latency of generative AI models as served through an inference server. For large language models (LLMs), GenAI-Perf provides metrics such as output token throughput, time to first token, time to second token, inter token latency, and request throughput.

**Key Features:**
- **Comprehensive LLM Metrics** — Output token throughput, time to first token, time to second token, and inter token latency
- **Request Performance** — Request throughput and latency measurements for inference servers
- **Modern Runtime** — Built with libc2.38+ for optimal performance and compatibility
- **Production Ready** — Designed for benchmarking production inference workloads

**What's Included:**

- **NVIDIA GenAI-Perf Tool** — Latest version with full metric collection capabilities
- **Runtime Dependencies** — libc2.38+ and all required system libraries
- **Performance Optimization** — Configured for accurate benchmarking measurements
- **Easy Integration** — Ready to connect to your inference servers

Perfect for performance testing and benchmarking your generative AI model deployments.

*For more specialized ML containers, check out our other templates in the `official-templates` directory.*
